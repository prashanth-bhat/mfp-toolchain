---
stages:
- name: Build-MFP-Server-Image
  inputs:
  - url: https://git.ng.bluemix.net/prashabh/mobilefoundationbyol.git
    type: git
    branch: master
    dir_name: null
  triggers:
  - type: commit
  jobs:
  - name: Build Docker image
    type: builder
    artifact_dir: output
    build_type: cr
    script: "#!/bin/bash\necho -e \"Build environment variables:\"\necho \"REGISTRY_URL=${REGISTRY_URL}\"\
      \necho \"REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}\"\necho \"IMAGE_NAME=${IMAGE_NAME}\"\
      \necho \"BUILD_NUMBER=${BUILD_NUMBER}\"\necho \"ARCHIVE_DIR=${ARCHIVE_DIR}\"\
      \n# Learn more about the available environment variables at:\n# https://console.bluemix.net/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment\n\
      \n# To review or change build options use:\n# bx cr build --help\nbuildImage()\n\
      {\necho -e \"Checking for Dockerfile at the repository bmx-kubernetes\"\nif\
      \ [ -f bmx-kubernetes/Dockerfile ]; then \n   echo \"Dockerfile found\"\nelse\n\
      \    echo \"Dockerfile not found\"\n    exit 1\nfi\n\necho -e \"Building container\
      \ image\"\nset -x\nbx cr build -t $REGISTRY_URL/$REGISTRY_NAMESPACE/$IMAGE_NAME:$BUILD_NUMBER\
      \ bmx-kubernetes/\nset +x\n}\n\n#######\nls\n\ncp -r dependencies bmx-kubernetes/dependencies\n\
      cp -r mfpf-libs bmx-kubernetes/mfpf-libs\ncp -r licenses bmx-kubernetes/licenses\n\
      cp -r common bmx-kubernetes/common\n\ncp bmx-kubernetes/Dockerfile-mfpf-server\
      \ bmx-kubernetes/Dockerfile\nbuildImage #server\n#cp bmx-kubernetes/Dockerfile-mfpf-analytics\
      \ bmx-kubernetes/Dockerfile\n#buildImage analytics\n######\n\n\n\n\n\nexport\
      \ PIPELINE_IMAGE_URL=\"$REGISTRY_URL/$REGISTRY_NAMESPACE/$IMAGE_NAME:$BUILD_NUMBER\"\
      \necho \"TODO - remove once no longer needed to unlock VA job ^^^^\"\n\nbx cr\
      \ images\n\necho \"==========================================================\"\
      \necho \"COPYING ARTIFACTS needed for deployment and testing (in particular\
      \ build.properties)\"\n\necho \"Checking archive dir presence\"\nmkdir -p $ARCHIVE_DIR\n\
      \n# Persist env variables into a properties file (build.properties) so that\
      \ all pipeline stages consuming this\n# build as input and configured with an\
      \ environment properties file valued 'build.properties'\n# will be able to reuse\
      \ the env variables in their job shell scripts.\n\n# CHART information from\
      \ build.properties is used in Helm Chart deployment to set the release name\n\
      #CHART_NAME=$(find chart/. -maxdepth 2 -type d -name '[^.]?*' -printf %f -quit)\n\
      echo \"CHART_NAME=${CHART_NAME}\" >> $ARCHIVE_DIR/build.properties\n# IMAGE\
      \ information from build.properties is used in Helm Chart deployment to set\
      \ the release name\necho \"IMAGE_NAME=${IMAGE_NAME}\" >> $ARCHIVE_DIR/build.properties\n\
      echo \"BUILD_NUMBER=${BUILD_NUMBER}\" >> $ARCHIVE_DIR/build.properties\n# REGISTRY\
      \ information from build.properties is used in Helm Chart deployment to generate\
      \ cluster secret\necho \"REGISTRY_URL=${REGISTRY_URL}\" >> $ARCHIVE_DIR/build.properties\n\
      echo \"REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}\" >> $ARCHIVE_DIR/build.properties\n\
      echo \"File 'build.properties' created for passing env variables to subsequent\
      \ pipeline jobs:\"\ncat $ARCHIVE_DIR/build.properties\n\necho \"Copy pipeline\
      \ scripts along with the build\"\n# Copy scripts (incl. deploy scripts)\nif\
      \ [ -d ./scripts/ ]; then\n  if [ ! -d $ARCHIVE_DIR/scripts/ ]; then # no need\
      \ to copy if working in ./ already\n    cp -r ./scripts/ $ARCHIVE_DIR/\n  fi\n\
      fi\n\necho \"Copy Helm chart along with the build\"\nif [ ! -d $ARCHIVE_DIR/chart/\
      \ ]; then # no need to copy if working in ./ already\n  cp -r ./chart/ $ARCHIVE_DIR/\n\
      fi\n\n"
    namespace: byol
    image_name: mobilefoundationbyol
    target:
      region_id: ibm:yp:us-south
      api_key_id: ApiKey-3c25572a-fbd4-44b5-886b-666b3f2172ad
- name: Deploy-MFP-Server-Helm
  inputs:
  - type: job
    stage: Build-MFP-Server-Image
    job: Build Docker image
    dir_name: null
  triggers:
  - type: stage
  properties:
  - name: buildprops
    value: build.properties
    type: file
  - name: CLUSTER_NAMESPACE
    value: default
    type: text
  - name: CHART_NAME
    value: ibm-mfpf-server-prod
    type: text
  - name: REQUESTS_CPU
    value: 1000m
    type: text
  - name: REQUESTS_MEMORY
    value: 2048M
    type: text
  - name: LIMITS_CPU
    value: 2000m
    type: text
  - name: LIMITS_MEMORY
    value: 4096M
    type: text
  - name: USER
    value: admin
    type: text
  - name: PASSWORD
    value: admin
    type: text
  - name: DBHOST
    value: dashdb-txn-small-yp-dal09-01.services.dal.bluemix.net
    type: text
  - name: DBPORT
    value: '50000'
    type: text
  - name: DBUSER
    value: bluadmin
    type: text
  - name: DBPASSWORD
    value: NTZjYWJkYjdjNmQ3
    type: text
  - name: DB
    value: BLUDB
    type: text
  - name: SCHEMA
    value: DEVOPSBYOL
    type: text
  - name: REPLICAS
    value: '2'
    type: text
  jobs:
  - name: Pre-deploy check
    type: deployer
    deploy_permission: DEV_IN_SPACE
    deploy_type: kubernetes
    target:
      region_id: ibm:yp:us-south
      api_key_id: ApiKey-3c25572a-fbd4-44b5-886b-666b3f2172ad
      kubernetes_cluster: mf-service-dev
    script: "#!/bin/bash\n# uncomment to debug the script\n#set -x\n# copy the script\
      \ below into your app code repo (e.g. ./scripts/check_predeploy.sh) and 'source'\
      \ it from your pipeline job\n#    source ./scripts/check_predeploy.sh\n# alternatively,\
      \ you can source it from online script:\n#    source <(curl -sSL \"https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_predeploy.sh\"\
      )\n# ------------------\n# source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_predeploy.sh\n\
      # Input env variables (can be received via a pipeline environment properties.file.\n\
      echo \"CHART_NAME=${CHART_NAME}\"\necho \"IMAGE_NAME=${IMAGE_NAME}\"\necho \"\
      BUILD_NUMBER=${BUILD_NUMBER}\"\necho \"REGISTRY_URL=${REGISTRY_URL}\"\necho\
      \ \"REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}\"\n      #View build properties\n\
      # cat build.properties\n# also run 'env' command to find all available env variables\n\
      # or learn more about the available environment variables at:\n# https://console.bluemix.net/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment\n\
      \n# Input env variables from pipeline job\necho \"PIPELINE_KUBERNETES_CLUSTER_NAME=${PIPELINE_KUBERNETES_CLUSTER_NAME}\"\
      \necho \"CLUSTER_NAMESPACE=${CLUSTER_NAMESPACE}\"\n\n#Check cluster availability\n\
      echo \"==========================================================\"\necho \"\
      CHECKING CLUSTER readiness and namespace existence\"\nIP_ADDR=$(bx cs workers\
      \ ${PIPELINE_KUBERNETES_CLUSTER_NAME} | grep normal | awk '{ print $2 }')\n\
      if [ -z \"${IP_ADDR}\" ]; then\n  echo -e \"${PIPELINE_KUBERNETES_CLUSTER_NAME}\
      \ not created or workers not ready\"\n  exit 1\nfi\necho \"Configuring cluster\
      \ namespace\"\nif kubectl get namespace ${CLUSTER_NAMESPACE}; then\n  echo -e\
      \ \"Namespace ${CLUSTER_NAMESPACE} found.\"\nelse\n  kubectl create namespace\
      \ ${CLUSTER_NAMESPACE}\n  echo -e \"Namespace ${CLUSTER_NAMESPACE} created.\"\
      \nfi\n\n# Grant access to private image registry from namespace $CLUSTER_NAMESPACE\n\
      # reference https://console.bluemix.net/docs/containers/cs_cluster.html#bx_registry_other\n\
      echo \"==========================================================\"\necho -e\
      \ \"CONFIGURING ACCESS to private image registry from namespace ${CLUSTER_NAMESPACE}\"\
      \nIMAGE_PULL_SECRET_NAME=\"ibmcloud-toolchain-${PIPELINE_TOOLCHAIN_ID}-${REGISTRY_URL}\"\
      \necho -e \"Checking for presence of ${IMAGE_PULL_SECRET_NAME} imagePullSecret\
      \ for this toolchain\"\nif ! kubectl get secret ${IMAGE_PULL_SECRET_NAME} --namespace\
      \ ${CLUSTER_NAMESPACE}; then\n  echo -e \"${IMAGE_PULL_SECRET_NAME} not found\
      \ in ${CLUSTER_NAMESPACE}, creating it\"\n  # for Container Registry, docker\
      \ username is 'token' and email does not matter\n  kubectl --namespace ${CLUSTER_NAMESPACE}\
      \ create secret docker-registry ${IMAGE_PULL_SECRET_NAME} --docker-server=${REGISTRY_URL}\
      \ --docker-password=${PIPELINE_BLUEMIX_API_KEY} --docker-username=iamapikey\
      \ --docker-email=a@b.com\nelse\n  echo -e \"Namespace ${CLUSTER_NAMESPACE} already\
      \ has an imagePullSecret for this toolchain.\"\nfi\necho \"Checking ability\
      \ to pass pull secret via Helm chart\"\nCHART_PULL_SECRET=$( grep 'pullSecret'\
      \ ./chart/${CHART_NAME}/values.yaml || : )\nif [ -z \"$CHART_PULL_SECRET\" ];\
      \ then\n  echo \"WARNING: Chart is not expecting an explicit private registry\
      \ imagePullSecret. Will patch the cluster default serviceAccount to pass it\
      \ implicitly for now.\"\n  echo \"Going forward, you should edit the chart to\
      \ add in:\"\n  echo -e \"[./chart/${CHART_NAME}/templates/deployment.yaml] (under\
      \ kind:Deployment)\"\n  echo \"    ...\"\n  echo \"    spec:\"\n  echo \"  \
      \    imagePullSecrets:               #<<<<<<<<<<<<<<<<<<<<<<<<\"\n  echo \"\
      \        - name: __not_implemented__   #<<<<<<<<<<<<<<<<<<<<<<<<\"\n  echo \"\
      \      containers:\"\n  echo \"        - name: __not_implemented__\"\n  echo\
      \ \"          image: \"__not_implemented__:__not_implemented__\"\n  echo \"\
      \    ...\"          \n  echo -e \"[./chart/${CHART_NAME}/values.yaml]\"\n  echo\
      \ \"or check out this chart example: https://github.com/open-toolchain/hello-helm/tree/master/chart/hello\"\
      \n  echo \"or refer to: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/#create-a-pod-that-uses-your-secret\"\
      \n  echo \"    ...\"\n  echo \"    image:\"\n  echo \"repository: webapp\"\n\
      \  echo \"  tag: 1\"\n  echo \"  pullSecret: regsecret            #<<<<<<<<<<<<<<<<<<<<<<<<\"\
      \"\n  echo \"  pullPolicy: IfNotPresent\"\n  echo \"    ...\"\n  echo \"Enabling\
      \ default serviceaccount to use the pull secret\"\n  kubectl patch -n ${CLUSTER_NAMESPACE}\
      \ serviceaccount/default -p '{\"imagePullSecrets\":[{\"name\":\"'\"${IMAGE_PULL_SECRET_NAME}\"\
      '\"}]}'\n  echo \"default serviceAccount:\"\n  kubectl get serviceAccount default\
      \ -o yaml\n  echo -e \"Namespace ${CLUSTER_NAMESPACE} authorizing with private\
      \ image registry using patched default serviceAccount\"\nelse\n  echo -e \"\
      Namespace ${CLUSTER_NAMESPACE} authorizing with private image registry using\
      \ Helm chart imagePullSecret\"\nfi\n\necho \"==========================================================\"\
      \necho \"CONFIGURING TILLER enabled (Helm server-side component)\"\nhelm init\
      \ --upgrade\nkubectl rollout status -w deployment/tiller-deploy --namespace=kube-system\n\
      helm version\n\necho \"==========================================================\"\
      \necho -e \"CHECKING HELM releases in this namespace: ${CLUSTER_NAMESPACE}\"\
      \nhelm list --namespace ${CLUSTER_NAMESPACE}\n"
  - name: Deploy Helm chart
    type: deployer
    deploy_permission: DEV_IN_SPACE
    deploy_type: kubernetes
    target:
      region_id: ibm:yp:us-south
      api_key_id: ApiKey-3c25572a-fbd4-44b5-886b-666b3f2172ad
      kubernetes_cluster: mf-service-dev
    script: |-
      #!/bin/bash
      # uncomment to debug the script
      set -x
      # copy the script below into your app code repo (e.g. ./scripts/deploy_helm.sh) and 'source' it from your pipeline job
      #    source ./scripts/deploy_helm.sh
      # alternatively, you can source it from online script:
      #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/deploy_helm.sh")
      # ------------------
      # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/deploy_helm.sh
      # Input env variables (can be received via a pipeline environment properties.file.
      echo "CHART_NAME=${CHART_NAME}"
      echo "IMAGE_NAME=${IMAGE_NAME}"
      echo "BUILD_NUMBER=${BUILD_NUMBER}"
      echo "REGISTRY_URL=${REGISTRY_URL}"
      echo "REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}"

      #View build properties
      # cat build.properties
      # also run 'env' command to find all available env variables
      # or learn more about the available environment variables at:
      # https://console.bluemix.net/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment

      # Input env variables from pipeline job
      echo "PIPELINE_KUBERNETES_CLUSTER_NAME=${PIPELINE_KUBERNETES_CLUSTER_NAME}"
      echo "CLUSTER_NAMESPACE=${CLUSTER_NAMESPACE}"

      echo "=========================================================="
      echo "DEFINE RELEASE by prefixing image (app) name with namespace if not 'default' as Helm needs unique release names across namespaces"
      if [[ "${CLUSTER_NAMESPACE}" != "default" ]]; then
        RELEASE_NAME="${CLUSTER_NAMESPACE}-${IMAGE_NAME}"
      else
        RELEASE_NAME=${IMAGE_NAME}
      fi
      echo -e "Release name: ${RELEASE_NAME}"

      echo "=========================================================="
      echo "DEPLOYING HELM chart"
      IMAGE_REPOSITORY=${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}
      IMAGE_PULL_SECRET_NAME="ibmcloud-toolchain-${PIPELINE_TOOLCHAIN_ID}-${REGISTRY_URL}"
      INGRESS_HOST=`echo $(bx cs cluster-get ${PIPELINE_KUBERNETES_CLUSTER_NAME} | grep "Ingress Subdomain" | cut -d":" -f2)`

      # Using 'upgrade --install" for rolling updates. Note that subsequent updates will occur in the same namespace the release is currently deployed in, ignoring the explicit--namespace argument".
      echo -e "Dry run into: ${PIPELINE_KUBERNETES_CLUSTER_NAME}/${CLUSTER_NAMESPACE}."
      helm upgrade --install --debug --dry-run ${RELEASE_NAME} ./chart/${CHART_NAME} --set image.name=${IMAGE_REPOSITORY},image.tag=${BUILD_NUMBER},image.pullSecret=${IMAGE_PULL_SECRET_NAME},resources.requests.cpu=${REQUESTS_CPU},resources.requests.memory=${REQUESTS_MEMORY},resources.limits.cpu=${LIMITS_CPU},resources.limits.memory=${LIMITS_MEMORY},mobileFirstOperationsConsole.user=${USER},mobileFirstOperationsConsole.password=${PASSWORD},existingDB2Details.db2Host=${DBHOST},existingDB2Details.db2Port=${DBPORT},existingDB2Details.db2Username=${DBUSER},existingDB2Details.db2Password=${DBPASSWORD},existingDB2Details.db2Database=${DB},existingDB2Details.db2Schema=${SCHEMA},scaling.replicaCount=${REPLICAS},ingress.sslSecretName=${PIPELINE_KUBERNETES_CLUSTER_NAME},ingress.host=${INGRESS_HOST} --namespace ${CLUSTER_NAMESPACE}

      echo -e "Deploying into: ${PIPELINE_KUBERNETES_CLUSTER_NAME}/${CLUSTER_NAMESPACE}."
      helm upgrade  --install ${RELEASE_NAME} ./chart/${CHART_NAME} --set image.name=${IMAGE_REPOSITORY},image.tag=${BUILD_NUMBER},image.pullSecret=${IMAGE_PULL_SECRET_NAME},resources.requests.cpu=${REQUESTS_CPU},resources.requests.memory=${REQUESTS_MEMORY},resources.limits.cpu=${LIMITS_CPU},resources.limits.memory=${LIMITS_MEMORY},mobileFirstOperationsConsole.user=${USER},mobileFirstOperationsConsole.password=${PASSWORD},existingDB2Details.db2Host=${DBHOST},existingDB2Details.db2Port=${DBPORT},existingDB2Details.db2Username=${DBUSER},existingDB2Details.db2Password=${DBPASSWORD},existingDB2Details.db2Database=${DB},existingDB2Details.db2Schema=${SCHEMA},scaling.replicaCount=${REPLICAS},ingress.sslSecretName=${PIPELINE_KUBERNETES_CLUSTER_NAME},ingress.host=${INGRESS_HOST} --namespace ${CLUSTER_NAMESPACE}

      echo "=========================================================="
      echo -e "CHECKING deployment status of release ${RELEASE_NAME} with image tag: ${BUILD_NUMBER}"
      echo ""
      for ITERATION in {1..30}
      do
        DATA=$( kubectl get pods --namespace ${CLUSTER_NAMESPACE} -a -l release=${RELEASE_NAME} -o json )
        NOT_READY=$( echo $DATA | jq '.items[].status.containerStatuses[] | select(.image=="'"${IMAGE_REPOSITORY}:${BUILD_NUMBER}"'") | select(.ready==false) ' )
        if [[ -z "$NOT_READY" ]]; then
          echo -e "All pods are ready:"
          echo $DATA | jq '.items[].status.containerStatuses[] | select(.image=="'"${IMAGE_REPOSITORY}:${BUILD_NUMBER}"'") | select(.ready==true) '
          break # deployment succeeded
        fi
        REASON=$(echo $DATA | jq '.items[].status.containerStatuses[] | select(.image=="'"${IMAGE_REPOSITORY}:${BUILD_NUMBER}"'") | .state.waiting.reason')
        echo -e "${ITERATION} : Deployment still pending..."
        echo -e "NOT_READY:${NOT_READY}"
        echo -e "REASON: ${REASON}"
        if [[ ${REASON} == *ErrImagePull* ]] || [[ ${REASON} == *ImagePullBackOff* ]]; then
          echo "Detected ErrImagePull or ImagePullBackOff failure. "
          echo "Please check proper authenticating to from cluster to image registry (e.g. image pull secret)"
          break; # no need to wait longer, error is fatal
        elif [[ ${REASON} == *CrashLoopBackOff* ]]; then
          echo "Detected CrashLoopBackOff failure. "
          echo "Application is unable to start, check the application startup logs"
          break; # no need to wait longer, error is fatal
        fi
        sleep 5
      done

      if [[ ! -z "$NOT_READY" ]]; then
        echo ""
        echo "=========================================================="
        echo "DEPLOYMENT FAILED"
        echo "Deployed Services:"
        kubectl describe services ${RELEASE_NAME}-${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
        echo ""
        echo "Deployed Pods:"
        kubectl describe pods --selector app=${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
        echo ""
        echo "Application Logs"
        kubectl logs --selector app=${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
        echo "=========================================================="
        PREVIOUS_RELEASE=$( helm history ${RELEASE_NAME} | grep SUPERSEDED | sort -r -n | awk '{print $1}' | head -n 1 )
        echo -e "Could rollback to previous release: ${PREVIOUS_RELEASE} using command:"
        echo -e "helm rollback ${RELEASE_NAME} ${PREVIOUS_RELEASE}"
        # helm rollback ${RELEASE_NAME} ${PREVIOUS_RELEASE}
        # echo -e "History for release:${RELEASE_NAME}"
        # helm history ${RELEASE_NAME}
        # echo "Deployed Services:"
        # kubectl describe services ${RELEASE_NAME}-${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
        # echo ""
        # echo "Deployed Pods:"
        # kubectl describe pods --selector app=${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
        exit 1
      fi

      echo ""
      echo "=========================================================="
      echo "DEPLOYMENT SUCCEEDED"
      echo ""
      echo -e "Status for release:${RELEASE_NAME}"
      helm status ${RELEASE_NAME}

      echo ""
      echo -e "History for release:${RELEASE_NAME}"
      helm history ${RELEASE_NAME}

      # echo ""
      # echo "Deployed Services:"
      # kubectl describe services ${RELEASE_NAME}-${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}
      # echo ""
      # echo "Deployed Pods:"
      # kubectl describe pods --selector app=${CHART_NAME} --namespace ${CLUSTER_NAMESPACE}

      echo "=========================================================="
      IP_ADDR=$(bx cs workers ${PIPELINE_KUBERNETES_CLUSTER_NAME} | grep normal | head -n 1 | awk '{ print $2 }')
      PORT=$(kubectl get services --namespace ${CLUSTER_NAMESPACE} | grep ${RELEASE_NAME} | sed 's/.*:\([0-9]*\).*/\1/g')
      echo -e "View the application at: http://${INGRESS_HOST}/mfpconsole"
hooks:
- enabled: true
  label: null
  ssl_enabled: false
  url: https://devops-api.ng.bluemix.net/v1/messaging/webhook/publish
